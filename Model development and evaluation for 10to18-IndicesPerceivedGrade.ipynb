{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "b83d8471",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, precision_score, recall_score, f1_score, confusion_matrix, accuracy_score\n",
    "\n",
    "def specificity(y_ture,y_pred):\n",
    "    MCM = confusion_matrix(y_ture, y_pred)\n",
    "    tn_sum = MCM[0, 0]\n",
    "    fp_sum = MCM[0, 1]\n",
    "\n",
    "    tp_sum = MCM[1, 1]\n",
    "    fn_sum = MCM[1, 0]\n",
    "\n",
    "    Condition_negative = tn_sum + fp_sum\n",
    "\n",
    "    Specificity = tn_sum / Condition_negative\n",
    "\n",
    "    return Specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "16da2684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n",
      "Fitting 5 folds for each of 940 candidates, totalling 4700 fits\n"
     ]
    }
   ],
   "source": [
    "#用oversampling的方法进行建模\n",
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import pickle as pl \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score,make_scorer\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import numpy as np\n",
    "import pandas\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "file_afterlabel = pandas.read_excel(\"The Shanghai data with labels.xlsx\")\n",
    "\n",
    "\n",
    "file1_1_afterlabel = file_afterlabel.loc[file_afterlabel[\"Sex\"]==0]#select the female\n",
    "file2_1_afterlabel = file_afterlabel.loc[file_afterlabel[\"Sex\"]==1]#select the male\n",
    "\n",
    "\n",
    "file1_1_afterlabel_years = [file1_1_afterlabel.loc[round(file1_1_afterlabel[\"Age\"]) == i] for i in range(10,19)]\n",
    "file2_1_afterlabel_years = [file2_1_afterlabel.loc[round(file2_1_afterlabel[\"Age\"]) == i] for i in range(10,19)]\n",
    "#oversampling\n",
    "for k in range(9):\n",
    "    x_lin = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==1]\n",
    "    num_range = x_lin.shape[0]\n",
    "    x_lin.index = range(num_range)\n",
    "    file1_1_afterlabel_years[k] = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==0]\n",
    "    num_oversampling = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==0].shape[0]\n",
    "    for i in range(num_oversampling):\n",
    "        x_lin_lin = x_lin.loc[np.round(random.uniform(0,num_range-1))]\n",
    "        file1_1_afterlabel_years[k] = pandas.concat([file1_1_afterlabel_years[k],pandas.DataFrame(x_lin_lin).T],axis=0)    \n",
    "\n",
    "for k in range(9):\n",
    "    x_lin = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==1]\n",
    "    num_range = x_lin.shape[0]\n",
    "    x_lin.index = range(num_range)\n",
    "    file2_1_afterlabel_years[k] = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==0]\n",
    "    num_oversampling = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==0].shape[0]\n",
    "    for i in range(num_oversampling):\n",
    "        x_lin_lin = x_lin.loc[np.round(random.uniform(0,num_range-1))]\n",
    "        file2_1_afterlabel_years[k] = pandas.concat([file2_1_afterlabel_years[k],pandas.DataFrame(x_lin_lin).T],axis=0)    \n",
    "\n",
    "#re-order for the nexe extraction\n",
    "for k in range(9):\n",
    "    file1_1_afterlabel_years[k].index = range(file1_1_afterlabel_years[k].shape[0])\n",
    "    file2_1_afterlabel_years[k].index = range(file2_1_afterlabel_years[k].shape[0])\n",
    "    \n",
    "    \n",
    "X_12_0_new_afterlabel_years = [file1_1_afterlabel_years[i].loc[:][file1_1_afterlabel_years[i].columns[1:23]] for i in range(9)]\n",
    "X_12_1_new_afterlabel_years = [file2_1_afterlabel_years[i].loc[:][file2_1_afterlabel_years[i].columns[1:23]] for i in range(9)]\n",
    "\n",
    "\n",
    "\n",
    "y_12_0_new_afterlabel_years = [file1_1_afterlabel_years[i].loc[:][\"Label\"]for i in range(9)]\n",
    "y_12_1_new_afterlabel_years = [file2_1_afterlabel_years[i].loc[:][\"Label\"]for i in range(9)]\n",
    "\n",
    "#construct the empty list\n",
    "X_12_0_new_afterlabel_train = [[] for i in range(9)]  \n",
    "X_12_0_new_afterlabel_test  = [[] for i in range(9)]\n",
    "y_12_0_new_afterlabel_train = [[] for i in range(9)]\n",
    "y_12_0_new_afterlabel_test = [[] for i in range(9)]\n",
    "\n",
    "X_12_1_new_afterlabel_train = [[] for i in range(9)]  \n",
    "X_12_1_new_afterlabel_test  = [[] for i in range(9)]\n",
    "y_12_1_new_afterlabel_train = [[] for i in range(9)]\n",
    "y_12_1_new_afterlabel_test = [[] for i in range(9)]\n",
    "\n",
    "for i in range(9):\n",
    "    X_12_0_new_afterlabel_train[i],X_12_0_new_afterlabel_test[i],y_12_0_new_afterlabel_train[i],y_12_0_new_afterlabel_test[i] = train_test_split(\n",
    "    X_12_0_new_afterlabel_years[i],y_12_0_new_afterlabel_years[i],test_size=0.3,random_state=2022,stratify=y_12_0_new_afterlabel_years[i])\n",
    "\n",
    "#set random seed\n",
    "for i in range(9):\n",
    "    X_12_1_new_afterlabel_train[i],X_12_1_new_afterlabel_test[i],y_12_1_new_afterlabel_train[i],y_12_1_new_afterlabel_test[i] = train_test_split(\n",
    "    X_12_1_new_afterlabel_years[i],y_12_1_new_afterlabel_years[i],test_size=0.3,random_state=2022,stratify=y_12_1_new_afterlabel_years[i])\n",
    "\n",
    "\n",
    "ABC_10_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_10_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_11_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_11_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_12_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_12_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_13_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_13_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_14_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_14_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_15_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_15_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_16_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_16_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_17_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_17_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "\n",
    "ABC_18_0 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "ABC_18_1 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2, min_samples_split=20, min_samples_leaf=5),\n",
    "                         algorithm=\"SAMME.R\",\n",
    "                         n_estimators=10, learning_rate=1)\n",
    "#cross validation for hyperparameters\n",
    "param_dist = {\n",
    "        'n_estimators':range(3,50,1),\n",
    "        'learning_rate':np.linspace(0.01,2,20),\n",
    "        }\n",
    "\n",
    "grid_10_0 = GridSearchCV(ABC_10_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_10_1 = GridSearchCV(ABC_10_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_11_0 = GridSearchCV(ABC_11_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_11_1 = GridSearchCV(ABC_11_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_12_0 = GridSearchCV(ABC_12_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_12_1 = GridSearchCV(ABC_12_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_13_0 = GridSearchCV(ABC_13_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_13_1 = GridSearchCV(ABC_13_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_14_0 = GridSearchCV(ABC_14_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_14_1 = GridSearchCV(ABC_14_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_15_0 = GridSearchCV(ABC_15_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_15_1 = GridSearchCV(ABC_15_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_16_0 = GridSearchCV(ABC_16_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_16_1 = GridSearchCV(ABC_16_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_17_0 = GridSearchCV(ABC_17_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_17_1 = GridSearchCV(ABC_17_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_18_0 = GridSearchCV(ABC_18_0,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "grid_18_1 = GridSearchCV(ABC_18_1,param_dist,cv = 5, n_jobs = -1, verbose = 2)\n",
    "\n",
    "\n",
    "\n",
    "#training the models\n",
    "grid_10_0.fit(X_12_0_new_afterlabel_train[0],y_12_0_new_afterlabel_train[0].astype(\"int\"))\n",
    "grid_10_1.fit(X_12_1_new_afterlabel_train[0],y_12_1_new_afterlabel_train[0].astype(\"int\"))\n",
    "\n",
    "grid_11_0.fit(X_12_0_new_afterlabel_train[1],y_12_0_new_afterlabel_train[1].astype(\"int\"))\n",
    "grid_11_1.fit(X_12_1_new_afterlabel_train[1],y_12_1_new_afterlabel_train[1].astype(\"int\"))\n",
    "\n",
    "grid_12_0.fit(X_12_0_new_afterlabel_train[2],y_12_0_new_afterlabel_train[2].astype(\"int\"))\n",
    "grid_12_1.fit(X_12_1_new_afterlabel_train[2],y_12_1_new_afterlabel_train[2].astype(\"int\"))\n",
    "\n",
    "grid_13_0.fit(X_12_0_new_afterlabel_train[3],y_12_0_new_afterlabel_train[3].astype(\"int\"))\n",
    "grid_13_1.fit(X_12_1_new_afterlabel_train[3],y_12_1_new_afterlabel_train[3].astype(\"int\"))\n",
    "\n",
    "grid_14_0.fit(X_12_0_new_afterlabel_train[4],y_12_0_new_afterlabel_train[4].astype(\"int\"))\n",
    "grid_14_1.fit(X_12_1_new_afterlabel_train[4],y_12_1_new_afterlabel_train[4].astype(\"int\"))\n",
    "\n",
    "grid_15_0.fit(X_12_0_new_afterlabel_train[5],y_12_0_new_afterlabel_train[5].astype(\"int\"))\n",
    "grid_15_1.fit(X_12_1_new_afterlabel_train[5],y_12_1_new_afterlabel_train[5].astype(\"int\"))\n",
    "\n",
    "grid_16_0.fit(X_12_0_new_afterlabel_train[6],y_12_0_new_afterlabel_train[6].astype(\"int\"))\n",
    "grid_16_1.fit(X_12_1_new_afterlabel_train[6],y_12_1_new_afterlabel_train[6].astype(\"int\"))\n",
    "\n",
    "grid_17_0.fit(X_12_0_new_afterlabel_train[7],y_12_0_new_afterlabel_train[7].astype(\"int\"))\n",
    "grid_17_1.fit(X_12_1_new_afterlabel_train[7],y_12_1_new_afterlabel_train[7].astype(\"int\"))\n",
    "\n",
    "grid_18_0.fit(X_12_0_new_afterlabel_train[8],y_12_0_new_afterlabel_train[8].astype(\"int\"))\n",
    "grid_18_1.fit(X_12_1_new_afterlabel_train[8],y_12_1_new_afterlabel_train[8].astype(\"int\"))\n",
    "\n",
    "\n",
    "\n",
    "#save the trained models as pkl formats\n",
    "with open('grid_10_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_10_0, f)\n",
    "with open('grid_10_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_10_1, f)\n",
    "    \n",
    "with open('grid_11_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_11_0, f)\n",
    "with open('grid_11_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_11_1, f)\n",
    "\n",
    "with open('grid_12_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_12_0, f)\n",
    "with open('grid_12_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_12_1, f)\n",
    "    \n",
    "with open('grid_13_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_13_0, f)\n",
    "with open('grid_13_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_13_1, f)\n",
    "    \n",
    "with open('grid_14_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_14_0, f)\n",
    "with open('grid_14_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_14_1, f)\n",
    "    \n",
    "with open('grid_15_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_15_0, f)\n",
    "with open('grid_15_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_15_1, f)\n",
    "    \n",
    "with open('grid_16_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_16_0, f)\n",
    "with open('grid_16_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_16_1, f)\n",
    "    \n",
    "with open('grid_17_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_17_0, f)\n",
    "with open('grid_17_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_17_1, f)\n",
    "    \n",
    "with open('grid_18_0.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_18_0, f)\n",
    "with open('grid_18_1.pkl', 'wb') as f:\n",
    "    pickle.dump(grid_18_1, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7acc358e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        召回率       精准率       准确率       特异性     AUC面积       f1值\n",
      "0  1.000000  0.538462  0.828571  0.785714  1.000000  0.700000\n",
      "1  1.000000  0.705882  0.919355  0.900000  0.980000  0.827586\n",
      "2  1.000000  0.640000  0.878378  0.844828  0.956897  0.780488\n",
      "3  1.000000  0.588235  0.898551  0.881356  0.977966  0.740741\n",
      "4  1.000000  0.666667  0.921569  0.906977  0.982558  0.800000\n",
      "5  1.000000  0.777778  0.950000  0.939394  0.995671  0.875000\n",
      "6  1.000000  0.454545  0.785714  0.739130  1.000000  0.625000\n",
      "7  0.666667  0.571429  0.800000  0.842105  0.947368  0.615385\n",
      "8  1.000000  0.571429  0.785714  0.700000  1.000000  0.727273\n",
      "   召回率       精准率       准确率       特异性     AUC面积       f1值\n",
      "0  1.0  0.714286  0.953488  0.947368  1.000000  0.833333\n",
      "1  1.0  0.600000  0.893333  0.873016  0.932540  0.750000\n",
      "2  1.0  0.523810  0.887640  0.871795  1.000000  0.687500\n",
      "3  1.0  0.666667  0.940594  0.932584  0.984082  0.800000\n",
      "4  1.0  0.545455  0.896907  0.882353  1.000000  0.705882\n",
      "5  1.0  0.526316  0.886076  0.869565  0.985507  0.689655\n",
      "6  1.0  0.642857  0.905660  0.886364  1.000000  0.782609\n",
      "7  1.0  0.700000  0.921053  0.903226  0.986175  0.823529\n",
      "8  1.0  1.000000  1.000000  1.000000  1.000000  1.000000\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import pickle\n",
    "import random\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "import pickle as pl \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_curve, auc \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#evaluate the performance of the series of talent prediction models\n",
    "\n",
    "file_afterlabel = pandas.read_excel(\"The Shanghai data with labels.xlsx\")\n",
    "\n",
    "\n",
    "file1_1_afterlabel = file_afterlabel.loc[file_afterlabel[\"Sex\"]==0]#select the female\n",
    "file2_1_afterlabel = file_afterlabel.loc[file_afterlabel[\"Sex\"]==1]#select the male\n",
    "\n",
    "#进行oversampling\n",
    "for k in range(9):\n",
    "    x_lin = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==1]\n",
    "    num_range = x_lin.shape[0]\n",
    "    x_lin.index = range(num_range)\n",
    "    file1_1_afterlabel_years[k] = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==0]\n",
    "    num_oversampling = file1_1_afterlabel_years[k].loc[file1_1_afterlabel_years[k][\"Label\"]==0].shape[0]\n",
    "    for i in range(num_oversampling):\n",
    "        x_lin_lin = x_lin.loc[np.round(random.uniform(0,num_range-1))]\n",
    "        file1_1_afterlabel_years[k] = pandas.concat([file1_1_afterlabel_years[k],pandas.DataFrame(x_lin_lin).T],axis=0)    \n",
    "\n",
    "for k in range(9):\n",
    "    x_lin = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==1]\n",
    "    num_range = x_lin.shape[0]\n",
    "    x_lin.index = range(num_range)\n",
    "    file2_1_afterlabel_years[k] = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==0]\n",
    "    num_oversampling = file2_1_afterlabel_years[k].loc[file2_1_afterlabel_years[k][\"Label\"]==0].shape[0]\n",
    "    for i in range(num_oversampling):\n",
    "        x_lin_lin = x_lin.loc[np.round(random.uniform(0,num_range-1))]\n",
    "        file2_1_afterlabel_years[k] = pandas.concat([file2_1_afterlabel_years[k],pandas.DataFrame(x_lin_lin).T],axis=0)    \n",
    "\n",
    "#re-order\n",
    "for k in range(9):\n",
    "    file1_1_afterlabel_years[k].index = range(file1_1_afterlabel_years[k].shape[0])\n",
    "    file2_1_afterlabel_years[k].index = range(file2_1_afterlabel_years[k].shape[0])\n",
    "\n",
    "index_name = file1_1_afterlabel.columns[1:23]\n",
    "\n",
    "file1_1_afterlabel_years = [file1_1_afterlabel.loc[round(file1_1_afterlabel[\"Age\"]) == i] for i in range(10,19)]#分别选出来10-18岁\n",
    "file2_1_afterlabel_years = [file2_1_afterlabel.loc[round(file2_1_afterlabel[\"Age\"]) == i] for i in range(10,19)]\n",
    "\n",
    "X_12_0_new_afterlabel_years = [file1_1_afterlabel_years[i].loc[:][file1_1_afterlabel_years[i].columns[1:23]] for i in range(9)]\n",
    "X_12_1_new_afterlabel_years = [file2_1_afterlabel_years[i].loc[:][file2_1_afterlabel_years[i].columns[1:23]] for i in range(9)]\n",
    "\n",
    "\n",
    "\n",
    "y_12_0_new_afterlabel_years = [file1_1_afterlabel_years[i].loc[:][\"Label\"]for i in range(9)]\n",
    "y_12_1_new_afterlabel_years = [file2_1_afterlabel_years[i].loc[:][\"Label\"]for i in range(9)]\n",
    "\n",
    "X_12_0_new_afterlabel_train = [[] for i in range(9)]  \n",
    "X_12_0_new_afterlabel_test  = [[] for i in range(9)]\n",
    "y_12_0_new_afterlabel_train = [[] for i in range(9)]\n",
    "y_12_0_new_afterlabel_test = [[] for i in range(9)]\n",
    "\n",
    "X_12_1_new_afterlabel_train = [[] for i in range(9)]  \n",
    "X_12_1_new_afterlabel_test  = [[] for i in range(9)]\n",
    "y_12_1_new_afterlabel_train = [[] for i in range(9)]\n",
    "y_12_1_new_afterlabel_test = [[] for i in range(9)]\n",
    "\n",
    "for i in range(9):\n",
    "    X_12_0_new_afterlabel_train[i],X_12_0_new_afterlabel_test[i],y_12_0_new_afterlabel_train[i],y_12_0_new_afterlabel_test[i] = train_test_split(\n",
    "    X_12_0_new_afterlabel_years[i],y_12_0_new_afterlabel_years[i],test_size=0.3,random_state=2022,stratify=y_12_0_new_afterlabel_years[i])\n",
    "\n",
    "#set random seed\n",
    "for i in range(9):\n",
    "    X_12_1_new_afterlabel_train[i],X_12_1_new_afterlabel_test[i],y_12_1_new_afterlabel_train[i],y_12_1_new_afterlabel_test[i] = train_test_split(\n",
    "    X_12_1_new_afterlabel_years[i],y_12_1_new_afterlabel_years[i],test_size=0.3,random_state=2022,stratify=y_12_1_new_afterlabel_years[i])\n",
    "\n",
    "y = [[] for i in range(9)]\n",
    "y_1 = [[] for i in range(9)]\n",
    "\n",
    "proba_test = [[] for i in range(9)]\n",
    "proba_test_1 = [[] for i in range(9)]\n",
    "\n",
    "y_pred = [[] for i in range(9)]\n",
    "y_pred_1 = [[] for i in range(9)]\n",
    "for i in range(9):\n",
    "    if i == 0:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_10_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_10_0.predict_proba(X_12_0_new_afterlabel_test[i])\n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_10_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_10_1.predict_proba(X_12_1_new_afterlabel_test[i])\n",
    "    if i == 1:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_11_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_11_0.predict_proba(X_12_0_new_afterlabel_test[i])        \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_11_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_11_1.predict_proba(X_12_1_new_afterlabel_test[i])\n",
    "    if i == 2:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_12_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_12_0.predict_proba(X_12_0_new_afterlabel_test[i])               \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_12_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_12_1.predict_proba(X_12_1_new_afterlabel_test[i])\n",
    "    if i == 3:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_13_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_13_0.predict_proba(X_12_0_new_afterlabel_test[i])            \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_13_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_13_1.predict_proba(X_12_1_new_afterlabel_test[i])        \n",
    "    if i == 4:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_14_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_14_0.predict_proba(X_12_0_new_afterlabel_test[i])               \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_14_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_14_1.predict_proba(X_12_1_new_afterlabel_test[i])            \n",
    "    if i == 5:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_15_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_15_0.predict_proba(X_12_0_new_afterlabel_test[i])                 \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_15_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_15_1.predict_proba(X_12_1_new_afterlabel_test[i])        \n",
    "    if i == 6:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_16_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_16_0.predict_proba(X_12_0_new_afterlabel_test[i])               \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_16_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_16_1.predict_proba(X_12_1_new_afterlabel_test[i])       \n",
    "    if i == 7:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_17_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_17_0.predict_proba(X_12_0_new_afterlabel_test[i])                      \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_17_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_17_1.predict_proba(X_12_1_new_afterlabel_test[i])     \n",
    "    if i == 8:\n",
    "        y[i] = y_12_0_new_afterlabel_test[i]\n",
    "        y_pred[i] = grid_18_0.predict(X_12_0_new_afterlabel_test[i])\n",
    "        proba_test[i] = grid_18_0.predict_proba(X_12_0_new_afterlabel_test[i])           \n",
    "        \n",
    "        y_1[i] = y_12_1_new_afterlabel_test[i]\n",
    "        y_pred_1[i] = grid_18_1.predict(X_12_1_new_afterlabel_test[i])\n",
    "        proba_test_1[i] = grid_18_1.predict_proba(X_12_1_new_afterlabel_test[i])     \n",
    "        \n",
    "for i in range(9):\n",
    "    roc_sum = [roc_curve(y[i],proba_test[i][:,1]) for i in range(9)]\n",
    "    fpr_test = [roc_sum[i][0] for i in range(9)]\n",
    "    tpr_test = [roc_sum[i][1] for i in range(9)]\n",
    "    threshold = [roc_sum[i][2] for i in range(9)]\n",
    "    roc_auc_test = [auc(fpr_test[i],tpr_test[i]) for i in range(9)]#calculate the AUC area\n",
    "    \n",
    "    roc_sum_1 = [roc_curve(y_1[i],proba_test_1[i][:,1]) for i in range(9)]\n",
    "    fpr_test_1 = [roc_sum_1[i][0] for i in range(9)]\n",
    "    tpr_test_1 = [roc_sum_1[i][1] for i in range(9)]\n",
    "    threshold_1 = [roc_sum_1[i][2] for i in range(9)]\n",
    "    roc_auc_test_1 = [auc(fpr_test_1[i],tpr_test_1[i]) for i in range(9)] \n",
    "        \n",
    "data_0 = pandas.DataFrame({\"Recall\":[recall_score(y[i], y_pred[i]) for i in range(9)],\"Precision\":[precision_score(y[i], y_pred[i]) for i in range(9)],\n",
    "                         \"Accuracy\":[accuracy_score(y[i], y_pred[i]) for i in range(9)],\"Specificity\":[specificity(y[i], y_pred[i]) for i in range(9)],\n",
    "                        \"AUC\":roc_auc_test,\"F1 score\":[f1_score(y[i], y_pred[i]) for i in range(9)]})\n",
    "data_1 = pandas.DataFrame({\"Recall\":[recall_score(y_1[i], y_pred_1[i]) for i in range(9)],\"Precision\":[precision_score(y_1[i], y_pred_1[i]) for i in range(9)],\n",
    "                         \"Accuracy\":[accuracy_score(y_1[i], y_pred_1[i]) for i in range(9)],\"Specificity\":[specificity(y_1[i], y_pred_1[i]) for i in range(9)],\n",
    "                         \"AUC\":roc_auc_test_1,\"F1 score\":[f1_score(y_1[i], y_pred_1[i]) for i in range(9)]})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
